---
title: "Bayesian mixed models with brms | session 1"
author: Jens Roeser 
date: 'Last updated: `r Sys.Date()`'
output: 
  ioslides_presentation:
#  revealjs::revealjs_presentation:
    theme: simplex
    highlight: zenburn
    incremental: true
    transition: slower
    widescreen: true
    smaller: true

bibliography: ["references.bib"]
csl: apa.csl
link-citations: no

---

```{r setup, include=FALSE}
library(citr)
library(tidyverse)
library(magrittr)
library(lme4)
library(brms)
library(ggthemes)
library(kableExtra)
library(knitr)
library(readxl)
library(extrafont)
library(broom)
library(tidybayes)
library(janitor)
source("../scripts/functions.R")
options("kableExtra.html.bsTable" = T)
knitr::opts_chunk$set(echo = FALSE,
                      comment=NA, 
                      warning = FALSE,
                      message =FALSE)
theme_set(theme_few(base_size = 12) )
```



## Before we start ...

>- Download the workshop folder from GitHub via RStudio.
>- Open RStudio $>$ `File` $>$ `New Project` $>$ `Version Control` $>$ `Git` $>$ paste url $>$ `Create Project`
>- https://github.com/jensroes/bayesian-mm-workshop.git
>- Exercise *R* scripts, data, slides
>- Create a folder called "stanout"


## Outline

>- Fit a Bayesian mixed effects model (diving in the deep end)
>- Why Bayesian and difference to (classical) frequentist approaches
>- Convergence and model diagnostics 
>- Evaluating the posterior
>- What are priors? (shouldn't this be the first item)
>- There will be exercises!!! (yay)



# Fitting a model in `brms` | [@brms1; @brms2; @burkner2019ordinal; @burkner2019bayesian]

## `brms` {.columns-2}

- *R* package for Bayesian models
- (Almost) no more complicated to fit than `lme4`s.
- More probability models than other regressions packages:
  - `gaussian`, `lognormal`, `bernoulli`, `poisson`, `zero_inflated_poisson`, `skew_normal`, `shifted_lognormal`, `exgaussian`, *et cetera*
  - and allows mixture models, nonlinear syntax, (un)equal variance signal-detection theory, multivariate models
  
\
  
- Under the hood: `brms` creates Stan code to compile a probabilistic MCMC (Markov Chain Monte Carlo) sampler.
- Compiling the sampler and for the sampler to obtain the full posterior can take times.






## Simulating data

<div style="float: left;width: 75%;">
```{r, eval = T, echo = T}
data <- jens_data_machine(intercept = 300, slope = 15)
```

```{r echo=F}
data %>%  
  arrange(participant_id, trial_id) %>%
  kable("html", digits = c(0,0,0,2),
        align = c("r", "r", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"), font_size = 16) %>% scroll_box(width = "100%", height = "360px")
```


</div>



## Simulating data

<div style="float: left;width: 75%">

```{r, eval = F, echo = T}
data <- jens_data_machine(intercept = 300, slope = 15)
```

```{r echo = F, fig.height = 4}
ggplot(data, aes(x = condition, y = y)) +
  geom_jitter(size = .1, alpha = .5, shape = 1, width = .1) +
  geom_boxplot(fill="transparent", width = .25, outlier.shape = NA) +
  scale_y_continuous(breaks = seq(0, 600, 150))
```

</div>

<div style="float: left;width: 25%">
> - You will use the same "data" in the exercises.
> - Think about it as keystroke intervals :)
> - We want to *uncover* the slope of 15 (difference between conditions) from the data.
> - After accounting for variability associated with the sampling process.

</div>



## Simulating data

```{r, eval = F, echo = T}
data <- jens_data_machine(intercept = 300, slope = 15)
```

```{r echo = F, fig.height = 4, fig.width=5.5}
data_sum <- data %>% group_by(participant_id, condition) %>%
  summarise(mean = mean(y)) %>%
  group_by(participant_id) %>%
  mutate(diff = ifelse(diff(mean)>0, TRUE, FALSE )) %>%
  group_by(condition) %>%
  mutate(N = n())

N <- unique(data_sum$N)

ggplot(data_sum, aes(x = condition, y = mean, colour = diff)) +
  geom_point(size = 3, alpha = .75, show.legend = F) +
  geom_path(aes(group = interaction(participant_id)), alpha = .5, show.legend = F) +
  labs(title = bquote("By-participant means ("*italic("N")==.(N)*")"), y = "y") +
  scale_color_colorblind()
```






## Formulating the model

$$
y_i \sim N(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \text{condition}_i\cdot\beta_1 + \text{participant}_i\\
$$

>- Outcome variable $y$
>- $N$: normal distribution with a mean $\mu$ and a residual (error) variance $\sigma^2$
>- Tilde symbol ($\sim$): "is distributed according to"
>- Equal sign ($=$): deterministic relationship
>- Subscript $i$: every observation in $1 \dots N$.
>- Greek symbols: unknown population parameter values (caret indicates estimated, e.g. $\hat{\beta}$)


## Formulating the model

$$
y_i \sim N(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \text{condition}_i\cdot\beta_1 + \text{participant}_i\\
$$

>- Treatment contrast: condition `a = 0`, condition `b = 1` 

```{r echo = T}
contrasts(factor(data$condition)) # default contrast
```

>- Given the data $y$ and this model, we can estimate $\beta_0$ and $\beta_1$.
>- For a Bayesian model we still need priors (end of session).
>- Intercept $\beta_0$: estimated mean of condition *a*.
>- Slope $\beta_1$: difference between condition *a* and *b*.
>- Participant variance with $N(0,\sigma_u)$


## *R* syntax

> - Frequentist mixed-effects models

```{r eval=F, echo = T}
library(lme4)
fit_lmer <- lmer(y ~ 1 + condition + (1|participant_id), data = data)
```

> - Bayesian mixed-effects models

```{r eval=F, echo = T}
library(brms)
fit_brm <- brm(y ~ 1 + condition + (1|participant_id), data = data)
```




## Exercise: fit models on simulated data

>- Complete and run scripts `lmer_sim.R` and `brms_sim.R`
>- Replace `---` correctly; run the script carefully line by line.
>- Check the output in the console.
>- Inspect and compare the fixed effects summaries.




## Fit models on simulated data

<div style="float: left;width: 75%; height:50%">

```{r, eval = F, echo = T}
data <- jens_data_machine(intercept = 300, slope = 15)
```

```{r, eval = T, echo = T}
fit_lmer <- lmer(y ~ 1 + condition + (1|participant_id), data = data)
```

```{r}
fit_brm <- readRDS(file = "../stanout/brms_sim.rda")
```

```{r, eval = F, echo = T}
fit_brm <- brm(y ~ 1 + condition + (1|participant_id), data = data)
```

</div>



## Fit models on simulated data

<div style="float: left;width: 75%">

```{r, eval = F, echo = T}
data <- jens_data_machine(intercept = 300, slope = 15)
```

```{r echo = T}
coef(summary(fit_lmer)) # Coefficients of frequentist model
```


```{r echo = T}
fixef(fit_brm) # Coefficients of Bayesian model
```

</div>




# Why go Bayesian? | see @kruschke2014doing. Full disclosure: I'm biased! 


## Two universes

>- Two different ways of generalising from sample to the population: 
>- What do the data tell us about the population?
>- **Null-hypothesis significance testing (NHST)**
>- **Bayesian**


## Null-hypothesis significance testing (NHST)


>- Classical / frequentist statistics; *p*-value based statistics
>- *p*-value: How plausible are the data (or something more extreme) if we assume that there's no effect?
>- Evaluating the probability of data (e.g. *t*-value, *F*-statistic) assuming that the null hypothesis $H_0$ is true.
>- If the data are extreme enough, $H_0$ is concluded to be implausible (rejected).
>- If $H_0$ is implausible, we assume $H_1$ (alternative hypothesis).
>- *Statistically significant* means data are unexpected under $H_0$.

$$
p(\text{data} \mid H_0 = \text{TRUE})
$$


## NHST and its problems {.columns-2}

- Statistical significance is not binary but continuous.
- How often are we actually interested in or seriously believe in $H_0$?
- What are you doing if $p=0.051$? 
- Stopping rule
- $\alpha$-level of 0.05 implies that there is a 5% chance that:
    - our effect doesn't replicate (isn't real) 
    - we will not observe an effect that is real
- This seems even worse in reality [@amrhein2019inferential;@loken2017measurement]

\

- Trade-off of statistical significance, effect size, and sample size: 
  - Smaller effects can become significant when the sample is large enough.
  - But how plausible is a small effect (e.g. mean difference, correlation) for what we are interested in?
  - What really matters is that the size of the effect is what we would expect it to be.
- Systematic misinterpretations of *p*-values [@colquhoun2017reproducibility; @greenland2016statistical] and CIs [@hoekstra2014robust; @morey2016fallacy].



## Convergence failure for `lmer` models 

\

`outcome ~ condition + (condition|participant) + (condition|item)` 

\

## Convergence failure for `lmer` models {.columns-2}

>- The statistical model should be an appropriate representation of the process that generates our data.
- Maximal random effects structure [@baayen2008mixed; @barr2013random]
- Overparametrisation: unidentifiable parameter estimates [@bates2015parsimonious]

\

- Solutions: reducing model complexity, changing optimizers
- Model selection based on failure to fit a more complex one.
- Some sources of random errors will not be addressed.
- Bayesian model converge *by definition* (as the number of iterations approaches $\infty$).



## Bayes' Theorem 

- Should I bring my umbrella? 
- I live in Nottingham: 122.1 rainy days per year
- I live in Los Angeles: 35.0 rainy days per year
- Forecast says 50% rain.
- My window says it's raining.
- Bayesian inference means updating one's belief about something as the new information becomes available.
- Bayesian inference is about uncertainty in belief in parameter values (e.g. a difference between means) and doesn't rely on fixed constants (e.g. *p*-values < 0.05).



## Bayes' Theorem 

$$
p(\theta \mid y) \propto p(\theta) \cdot p(y \mid \theta)
$$

>- $\theta$ = parameter(s), $y$ = data, $\mid$ = given (conditional on), $\propto$ = proportional to
>- $p(y \mid \theta)$ = likelihood (probability of data given model parameter values)
>- $p(\theta)$ = prior (what do we know about model parameter(s))
>- $p(\theta \mid y)$ = posterior distribution (our believe in different parameter values after seeing the data)



![](pics/bayes.jpeg)


## Bayes' Theorem 

$$
p(\theta \mid y) \propto p(\theta) \cdot p(y \mid \theta)
$$

>- Bayes' Theorem allows us to determine probability / uncertainty distributions of parameter values.
>- Frequentist models: maximum likelihood estimation we search for the parameters that maximize the log-likelihood.
>- Bayesian estimation uses the likelihood to update existing beliefs in different parameter values.
>- How much we believe in different parameter values depends on the data and what we already know.
>- We update our beliefs as new information becomes available.



## Why is Bayes important for linguistis (and everyone else)?

> It tells us what we want to know!

\

> Given the data (and our prior knowledge).

\

> - Summary stats of a Bayesian model are often very similar to what people *think* frequentist quantities (*p*-values, CIs) mean [@nicenboim2016statistical]:
> - What's the relative weight of evidence for one model (e.g., $H_0$) vs. another (e.g., $H_1$)?
> - What interval contains an unknown parameter value with .95 probability?


## Why is Bayes important for linguistis (and everyone else)?

> Instead of 

\

$$
p(\text{data} \mid H_0 = \text{TRUE})
$$

> we can 

\

$$
p(H \mid \text{data})
$$



## Why is Bayes important for linguistis (and everyone else)?

>- What's the probability of an effect given the data (and what we know)?
>- Useful for small data sets for which what we already know (prior information) plays a huge role.
>- Probabilisitic sampling can handle: convergence failures, missing data problems, complex models (many parameters, sources of random error), ceiling / floor effects, identifyability issues.
>- Results are not dependent on sampling plan [@kruschke2018rejecting].
>- Flexible data modeling is **now** easily available (`brms`, `rstanarm`, `rethinking`).



## Why is Bayes important for linguistis (and everyone else)?

>- Estimate credibility over parameter values based on the observed data [@kruschke2014doing].
>- Given the data, what parameter values should we believe in with most confidence.
>- For this we need to start with
1. a probability model (e.g. a normal distribution)
2. *prior* expectations (on e.g. means, difference, variability)
>- `brms` uses probabilistic sampling to determine the posterior uncertainty about the parameter value of the probability model given the prior and the data.
>- This involve calculating the weighted probability of the data given some parameter values (which can take time).







# Convergence and model diagnostics | see @lambert2018student for HMC


## So what about this?

<div style="float: left;width: 55%">

![](pics/sampling.png){width=100%}
</div>

<div style="float: right;width: 40%">

- Progress of probabilistic sampler.
- 2,000 iterations for parameter estimation per chain 
- `iterations / 2` warm-up samples: discarded eventually 
- 4 chains to establish convergence
- Change `cores` (check `parallel::detectCores()`) for running chains in parallel.
- How many posterior samples have we got for our inference (`= chains * (iterations - warm-up)`)?
- How many iterations / chains do you need?
</div>


## Parameter estimation

<div style="float: left;width: 40%">

>- **Hamiltonian Monte Carlo:** Markov chain Monte Carlo method for obtaining random samples.
>- Converge to being distributed according to a target probability distribution.
>- Probability of random samples is estimated given the data and the prior.
>- Direction changes if probability of proposed estimate is lower than previous one.
>- Parameter space is really multi-dimensional.

</div>


## Parameter estimation

<div style="float: left;width: 40%">

>- **Hamiltonian Monte Carlo:** Markov chain Monte Carlo method for obtaining random samples.
>- Converge to being distributed according to a target probability distribution.
>- Probability of random samples is estimated given the data and the prior.
>- Direction changes if probability of proposed estimate is lower than previous one.
>- Parameter space is really multi-dimensional.

</div>

<div style="float: right;width: 55%">

```{r }
sim <- fit_brm$fit@sim
samples <- map(1:4, ~sim$samples[[.]] %>%
                 as_tibble() %>%
      select(starts_with("b")) %>%
      mutate(chain = .x,
             iteration = 1:n())) %>% bind_rows() %>%
    pivot_longer(starts_with("b")) %>%
  mutate(name = factor(name, levels = unique(name)[c(1,2)], ordered = T),
         chain = factor(chain))
```

```{r fig.width=5}
pivot_wider(samples, names_from = name, values_from = value) %>%
  filter(chain == 1) %>%
  mutate(chain = paste0("Chain: ", chain)) %>%
  ggplot(aes(x = b_Intercept, y = b_conditionb, colour = chain)) +
  scale_color_viridis_d("") +
  geom_path(show.legend = F, colour =  "white") +
  facet_wrap(~chain) +
  labs(title = "Parameter space") +
  scale_x_continuous(limits = c(-100, 400), breaks = seq(-150, 600, 150)) +
  scale_y_continuous(limits = c(-50, 100), breaks = seq(-150, 600, 50)) +
  theme(legend.justification = "top")
```

</div>


## Parameter estimation

<div style="float: left;width: 40%">

>- **Hamiltonian Monte Carlo:** Markov chain Monte Carlo method for obtaining random samples.
>- Converge to being distributed according to a target probability distribution.
>- Probability of random samples is estimated given the data and the prior.
>- Direction changes if probability of proposed estimate is lower than previous one.
>- Parameter space is really multi-dimensional.

</div>

<div style="float: right;width: 55%">

```{r }

sim <- fit_brm$fit@sim
samples <- map(1:4, ~sim$samples[[.]] %>%
                 as_tibble() %>%
      select(starts_with("b")) %>%
      mutate(chain = .x,
             iteration = 1:n())) %>% bind_rows() %>%
    pivot_longer(starts_with("b")) %>%
  mutate(name = factor(name, levels = unique(name)[c(1,2)], ordered = T),
         chain = factor(chain))
```

```{r fig.width=5}
pivot_wider(samples, names_from = name, values_from = value) %>%
  filter(chain == 1) %>%
  mutate(chain = paste0("Chain: ", chain)) %>%
  ggplot(aes(x = b_Intercept, y = b_conditionb, colour = chain)) +
  scale_color_viridis_d("") +
  geom_path(show.legend = F) +
  facet_wrap(~chain) +
  labs(title = "Parameter space") +
  scale_x_continuous(limits = c(-100, 400), breaks = seq(-150, 600, 150)) +
  scale_y_continuous(limits = c(-50, 100), breaks = seq(-150, 600, 50)) +
  theme(legend.justification = "top")
```

</div>



## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1, iteration %in% 1:100) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```


## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1:2, iteration %in% 1:100) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```


## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1:4, iteration %in% 1:100) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```


## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1:4, iteration %in% 1:250) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```

## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1:4, iteration %in% 1:500) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```


## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1:4, iteration %in% 1:2000) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```

## Parameter estimation

```{r}
samples %>%
  filter(chain %in% 1:4, iteration %in% 1000:2000) %>%
  ggplot(aes(y=value, x = iteration, colour = chain)) +
  scale_color_viridis_d("Chains") +
  geom_path() +
  facet_wrap(~name, scales = "free") +
  labs(title = "Traceplot")
```

## Exercise: model diagnostics

>- Traceplots: we want fat hairy caterpillars 

```{r eval = F, echo = T}
plot(fit_brm, pars = c("b_Intercept", "b_conditionb"))
```


>- $\hat{R}$ convergence statistic; should be $<1.1$ [@gelman1992]

```{r eval = F, echo = T}
rhat(fit_brm, pars = c("b_Intercept", "b_conditionb")) 
```

>- Posterior predictive checks: compare observed data $y$ and model predictions $y_{rep}$

```{r eval=F, echo = T}
pp_check(fit_brm, nsamples = 100)
```

>- Replace the `---`s in scripts `model_diagnostics.R` and run the code (line by line).





# Posterior probability distribution | see e.g. @nicenboim2016statistical for a tutorial


## Posterior probability distribution


>- Get posterior of slope $\beta$ (difference between conditions)

```{r echo = T}
beta <- posterior_samples(fit_brm, pars = "b_conditionb") %>% pull()
```


```{r echo = T}
length(beta)
```


```{r echo = T}
beta[1:5]
```


## Posterior probability distribution


<div style="float: right;width: 55%">

```{r fig.height=4.5, fig.width=5.5}
ggplot(data = NULL, aes(x = beta)) +
  geom_histogram(alpha = .55) +
  labs(x = bquote("Estimated effect"~hat(beta)))
```

</div>

## Posterior probability distribution

<div style="float: left;width: 45%">

>- Posterior mean

```{r echo = T}
mean(beta)
```

</div>

<div style="float: right;width: 55%">

```{r fig.height=4.5, fig.width=5.5}
ggplot(data = NULL, aes(x = beta)) +
  geom_histogram(alpha = .55) +
  labs(x = bquote("Estimated effect"~hat(beta))) +
  geom_vline(xintercept = mean(beta), color = "darkred")
```
</div>


## Posterior probability distribution

<div style="float: left;width: 45%">

>- 95% probability interval (conceptually different from confidence interval)

```{r echo = T}
quantile(beta, probs = c(0.025, 0.975))
```


</div>

<div style="float: right;width: 55%">

```{r fig.height=4.5, fig.width=5.5}
ggplot(data = NULL, aes(x = beta)) +
  geom_histogram(alpha = .55) +
  labs(x = bquote("Estimated effect"~hat(beta))) +
  geom_errorbarh(aes(y = 65, xmin = quantile(beta, probs = c(0.025))
, xmax =  quantile(beta, probs = c(0.975))), color = "darkred", size = 1.5, ) +
  annotate("text", x = 15, y = 90, label = "95% PI", colour = "darkred")
```
</div>

## Posterior probability distribution

<div style="float: left;width: 45%">

>- 95% probability interval (conceptually different from confidence interval)

```{r echo = T}
quantile(beta, probs = c(0.025, 0.975))
```

>- 89% probability interval [@mcelreath2016statistical]

```{r echo = T}
quantile(beta, probs = c(0.055, 0.945))
```

</div>

<div style="float: right;width: 55%">

```{r fig.height=4.5, fig.width=5.5}
ggplot(data = NULL, aes(x = beta)) +
  geom_histogram(alpha = .55) +
  labs(x = bquote("Estimated effect"~hat(beta))) +
  geom_errorbarh(aes(y = 65, xmin = quantile(beta, probs = c(0.055))
, xmax =  quantile(beta, probs = c(0.945))), color = "darkred", size = 1.5, ) +
  annotate("text", x = 15, y = 90, label = "89% PI", colour = "darkred")
```
</div>


## Posterior probability distribution

<div style="float: left;width: 45%">

>- Probability that slope $\beta$ is negative: $P(\hat{\beta}<0)$

```{r echo = T}
mean(beta < 0)
```


</div>

<div style="float: right;width: 55%">

```{r fig.height=4.5, fig.width=5.5}
beta2 <- beta[beta < 0]
ggplot(data = NULL, aes(x = beta, fill = beta > -1)) +
  geom_histogram(alpha = .55, position = "identity", show.legend = F) +
  scale_fill_manual(values = c("darkred", "grey")) +
  geom_vline(xintercept = 0, colour = "grey20", linetype = "dotted") +
  labs(x = bquote("Estimated effect"~hat(beta))) 
```
</div>


## Posterior probability distribution

<div style="float: left;width: 45%">

>- Probability that slope $\beta$ is negative: $P(\hat{\beta}<0)$

```{r echo = T}
mean(beta < 0)
```

>- Probability that slope $\beta$ is smaller than 10: $P(\hat{\beta}<10)$

```{r echo = T}
mean(beta < 10)
```

>- More in the exercises: `parameter_evaluation.R`

</div>

<div style="float: right;width: 55%">

```{r fig.height=4.5, fig.width=5.5}
ggplot(data = NULL, aes(x = beta, fill = beta > 9.5)) +
  geom_histogram(alpha = .55,  position = "identity", show.legend = F) +
  geom_vline(xintercept = 10, colour = "grey20", linetype = "dotted") +
  scale_fill_manual(values = c("darkred", "grey")) +
  labs(x = bquote("Estimated effect"~hat(beta))) 
```
</div>



# What are priors? | chapter 7 in @lee2014bayesian


## Priors

$$
\text{posterior} \propto \text{prior} \cdot \text{likelihood}
$$

- Prior knowledge about plausible parameter values.
- This knowledge is expressed as probability distributions (e.g. normal distributions).
- Help probabilistic sampler by limiting the parameter space.
- Small data samples are sensitive to prior information which makes intuitively sense.
- Otherwise data typically overcome the prior (automatic Ockham's razor).
- Less common: test data against a (prior) effect suggested by the literature.
- We know more than we sometimes think!


## Priors: intercept

<div style="float: left;width: 40%;">

- *A priori*, each value in the parameter space is equally possible. 
- Let's think about the parameter space for models of keystroke data.

</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
tibble(intercept = runif(1000, -100000, 100000),
       slope = runif(1000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
  labs(subtitle = "Parameter space")
```
</div>

## Priors: intercept

<div style="float: left;width: 40%;">

>- Intercepts are some form of average.
>- Can keystroke intervals range between -$\infty$ and $\infty$? 
- What are the lower and upper end?

</div>


<div style="float: right;width: 50%;">

```{r fig.width=5}
tibble(intercept = runif(1000, -100000, 100000),
       slope = runif(1000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
  labs(subtitle = "Parameter space")
```
</div>


## Priors: intercept

<div style="float: left;width: 40%;">

>- Intercepts are some form of average.
>- Can keystroke intervals range between -$\infty$ and $\infty$? 
>- What are the lower and upper end?
- How long is the average pause before a sentence?


$$
\text{pre-sentence pause} \sim N(???, ???)
$$
</div>




<div style="float: right;width: 50%;">

```{r fig.width=5}
tibble(intercept = runif(1000, 0, 10000),
       slope = runif(1000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
  labs(subtitle = "Parameter space")
```

</div>


## Priors: intercept

<div style="float: left;width: 40%;">

>- Intercepts are some form of average.
>- Can keystroke intervals range between -$\infty$ and $\infty$? 
>- What are the lower and upper end?
- How long is the average pause before a sentence?


$$
\text{pre-sentence pause} \sim N(1000 \text{ msecs}, ???)
$$

</div>




<div style="float: right;width: 50%;">

```{r fig.width=5}
tibble(intercept = runif(1000, 0, 10000),
       slope = runif(1000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
  labs(subtitle = "Parameter space")
```

</div>


## Priors: intercept

<div style="float: left;width: 40%;">

>- Intercepts are some form of average.
>- Can keystroke intervals range between -$\infty$ and $\infty$? 
>- What are the lower and upper end?
- How long is the average pause before a sentence?
- How short / long can this be?
- Plausible probability distribution for pre-sentence pauses:

$$
\text{pre-sentence pause} \sim N(1000 \text{ msecs}, ???)
$$
</div>



<div style="float: right;width: 50%;">

```{r fig.width=5}
tibble(intercept = runif(1000, 0, 10000),
       slope = runif(1000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
  labs(subtitle = "Parameter space")
```

</div>

## Priors: intercept

<div style="float: left;width: 40%;">

>- Intercepts are some form of average.
>- Can keystroke intervals range between -$\infty$ and $\infty$? 
>- What are the lower and upper end?
- How long is the average pause before a sentence?
- How short / long can this be?
- Plausible probability distribution for pre-sentence pauses:


$$
\text{pre-sentence pause} \sim N(1000 \text{ msecs}, 500\text{ msecs})
$$
</div>


<div style="float: right;width: 50%;">

```{r fig.width=5}
tibble(intercept = runif(1000, 0, 10000),
       slope = runif(1000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
  labs(subtitle = "Parameter space")
```

</div>



## Priors: intercept

<div style="float: left;width: 40%;">

>- Intercepts are some form of average.
>- Can keystroke intervals range between -$\infty$ and $\infty$? 
>- What are the lower and upper end?
>- How long is the average pause before a sentence?
>- How short / long can this be?
- Plausible probability distribution for pre-sentence pauses:


$$
\text{pre-sentence pause} \sim N(1000 \text{ msecs}, 500\text{ msecs})
$$
</div>


<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>

## Priors: slope

<div style="float: left;width: 50%;">

- Say, we compare people writing in L1 and L2.
- Writing in L2 can be harder because of, e.g., lexical retrieval.
- Hence, pre-word / pre-sentence keystroke intervals are sometimes longer.
- What is a plausible prior for this delay?



</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>


## Priors: slope

<div style="float: left;width: 50%;">

>- Say, we compare people writing in L1 and L2.
>- Writing in L2 can be harder because of, e.g., lexical retrieval.
>- Hence, pre-word / pre-sentence keystroke intervals are sometimes longer.
>- What is a plausible prior for this delay?


$$
\text{slowdown for L2s} \sim N(???, ???)
$$
</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>



## Priors: slope

<div style="float: left;width: 50%;">

>- Say, we compare people writing in L1 and L2.
>- Writing in L2 can be harder because of, e.g., lexical retrieval.
>- Hence, pre-word / pre-sentence keystroke intervals are sometimes longer.
>- What is a plausible prior for this delay?


$$
\text{slowdown for L2s} \sim N(250 \text{ msecs}, 100\text{ msecs})
$$
</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>



## Priors: slope

<div style="float: left;width: 50%;">

>- Say, we compare people writing in L1 and L2.
>- Writing in L2 can be harder because of, e.g., lexical retrieval.
>- Hence, pre-word / pre-sentence keystroke intervals are sometimes longer.
>- What is a plausible prior for this delay?


$$
\text{slowdown for L2s} \sim N(250 \text{ msecs}, 100\text{ msecs})
$$
</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = rnorm(10000,  250, 100)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  scale_y_continuous(limits = c(-300,1000), breaks = seq(-300, 900, 300)) +
  labs(subtitle = "Parameter space")
ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "both")
```

</div>


## Priors: slope

<div style="float: left;width: 50%;">

- We often don't know what the effect could be.
- However, we have an intuition about what's plausible and what isn't.
- E.g., words with alternative spellings (*accordian*, *accordion*) may or may not lead to longer pauses (than words with less possible spellings; *aspergus*).
- We are not sure, so let's use a mean of 0 msecs; what would be a possible SD?

</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>

## Priors: slope

<div style="float: left;width: 50%;">

>- We often don't know what the effect could be.
>- However, we have an intuition about what's plausible and what isn't.
>- E.g., words with alternative spellings (*accordian*, *accordion*) may or may not lead to longer pauses (than words with less possible spellings; *aspergus*).
>- We are not sure, so let's use a mean of 0 msecs; what would be a possible SD?

$$
\text{slope} \sim N(0 \text{ msecs}, ???)
$$


</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>


## Priors: slope

<div style="float: left;width: 50%;">

>- We often don't know what the effect could be.
>- However, we have an intuition about what's plausible and what isn't.
>- E.g., words with alternative spellings (*accordian*, *accordion*) may or may not lead to longer pauses (than words with less possible spellings; *aspergus*).
>- We are not sure, so let's use a mean of 0 msecs; what would be a possible SD?


$$
\text{slope} \sim N(0 \text{ msecs}, 100\text{ msecs})
$$

>- We could even truncate the prior according to a lower / upper threshold.

</div>

<div style="float: right;width: 50%;">

```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 1000, sd = 500),
       slope = runif(10000,  -10000, 10000)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 10000)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "x")
```

</div>


## Priors: slope

<div style="float: left;width: 50%;">

>- We often don't know what the effect could be.
>- However, we have an intuition about what's plausible and what isn't.
>- E.g., words with alternative spellings (*accordian*, *accordion*) may or may not lead to longer pauses (than words with less possible spellings; *aspergus*).
>- We are not sure, so let's use a mean of 0 msecs; what would be a possible SD?


$$
\text{slope} \sim N(0 \text{ msecs}, 100\text{ msecs})
$$

>- We could even truncate the prior according to a lower / upper threshold.


</div>

<div style="float: right;width: 50%;">


```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 2000, sd = 1000),
       slope = rnorm(10000,  0, 100)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 20000)) +
  scale_y_continuous(limits = c(-300,1000), breaks = seq(-300, 900, 300)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "both")
```
</div>


## Priors

<div style="float: left;width: 50%;">

>- Even **weakly informative** priors are helpful for estimating parameter values.
>- They help to constrain the parameter space.
>- The parameter space should reflect our beliefs.
>- This is important for more complex models (mixtures, Wiener diffusion models).

</div>


<div style="float: right;width: 50%;">


```{r fig.width=5}
p <- tibble(intercept = rnorm(10000, mean = 2000, sd = 1000),
       slope = rnorm(10000,  0, 100)) %>%
  ggplot(aes(x=intercept, y=slope)) +
    geom_point(size = .25, colour = "transparent") + 
  geom_density_2d_filled(alpha = 0.25, show.legend = F) +
  geom_density_2d(alpha = 0.5, show.legend = F, size = .15, color = "black") +
  scale_fill_viridis_d(direction = -1, begin = 0, end = .6) +
  scale_x_continuous(limits = c(0, 20000)) +
  scale_y_continuous(limits = c(-300,1000), breaks = seq(-300, 900, 300)) +
  labs(subtitle = "Parameter space")

ggExtra::ggMarginal(p, type="density", size=10, alpha = .25, margins = "both")
```
</div>

## Priors

> Check defaults used earlier:

\

```{r echo = T, eval = F}
fit_brm <- readRDS(file = "../stanout/brms_sim.rda")
prior_summary(fit_brm)
```

```{r echo = F, eval = T}
fit_brm <- readRDS(file = "../stanout/brms_sim.rda")
prior <- prior_summary(fit_brm) # %>% as_data_frame() %>% select(-source)
sd_prior <- prior %>% filter(class == "sd" & group == "") %>% pull(prior)
prior %>% as_tibble() %>%
  mutate(prior = ifelse(class == "b", "(flat)", prior),
         prior = ifelse(class == "sd", sd_prior, prior)) %>%
  select(prior:group) %>% kable() %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, width = "20em") %>%
  column_spec(2:4, width = "10em") 
```



## Student *t*-distribution

<div style="float: left;width: 35%;">

>- Symmetric continuous distribution with fat tails assigning more probability to extreme values.
>- $\nu$ parameter controls the degrees of freedom 
>- $\nu = 1$ is a Cauchy distribution 
>- When $\nu \rightarrow \infty$ the *t*-distribution becomes Gaussian.

</div>


<div style="float: right;width: 60%;">

```{r fig.width=6}
library(LaplacesDemon)

x <- seq(-10, 10, by = 0.1)

tibble("t0" = dnorm(x, mean=0, sd=1),
       "t1" = dst(x, mu=0, sigma=1, nu = 1),
       "t2" = dst(x, mu=0, sigma=1, nu = 5),
       "t3" = dst(x, mu=0, sigma=2, nu = 5),
       x = x) %>%
  pivot_longer(-x) %>%
  ggplot(aes(x = x, y = value, colour = name)) +
  geom_line() +
  scale_x_continuous(limits = c(-7, 7)) +
  scale_color_colorblind("Parameter values",
                         breaks = paste0("t",0:3), 
                         labels = c(bquote(mu==0*","~sigma==1),
                                    bquote(mu==0*","~sigma==1*","~nu==1),
                                    bquote(mu==0*","~sigma==1*","~nu==5),
                                    bquote(mu==0*","~sigma==2*","~nu==5))) +
  labs(x = "x", y = "density") +
  theme(legend.justification = "top") 

```
</div>

## Model specification


> Instead of 

\

```{r echo = T, eval = F}
fit <- brm(outcome ~ predictor + (1|participant), data = data)
```

\

> do ...

\


```{r echo = T, eval = F}
# Create model
model <- bf(outcome ~ predictor + (1|participant))
# specifying model formula outside of brms works for lmer too.
# bf = brmsformula

# Fit brms
fit <- brm(model, data = data)
```



## Prior specification


```{r echo = T, eval = F}
# Create model
model <- bf(outcome ~ predictor + (1|participant))

# Look at priors: some have reasonable defaults, others are flat.
get_prior(model, data = data)

# Specify priors
prior <- set_prior("normal(1000, 500)", class = "Intercept", lb = 0, ub = 100000) +
         set_prior("normal(0, 100)", class = "b") # for slope(s)

# Fit brms
fit <- brm(model, data = data, prior = prior)
```





## Exercise: priors

>- Adding priors to last session's model in `brms_sim_with_prior.R`; replace the `---`s according to the comments in the script.
>- If you have time, make the standard deviation of the prior for slope $b$ either much larger or smaller and check how this affects the posterior estimate.


```{r}
all_dists <- read_csv("../data/prior_simulation.csv") %>%
  mutate(prior_id = rep(c(1,4,3,5,2), n()/5),
         group = recode(group, data = "likelihood")) %>%
  mutate(prior = paste0("prior ~ ", prior),
         prior = factor(prior, levels = prior[unique(prior_id)], ordered = T),
         group = factor(group, levels = c("likelihood", "prior", "posterior"), ordered = T))
```



## Prior simulation

```{r}
all_dists %>%
  ggplot(aes(x = x, y = y, colour = group, fill = group)) +  facet_wrap(~prior) +
  geom_vline(xintercept = 15, linetype = "dotted") +
  geom_line(colour = "transparent", fill = "transparent") +
  geom_ribbon(aes(ymax = y, ymin = 0), alpha = .25, colour = "transparent", fill = "transparent") +
  scale_fill_colorblind("") +
  scale_color_colorblind("") +
  scale_y_continuous(limits = c(0, 0.065)) +
  scale_x_continuous(limits = c(-90, 50)) +
  theme(legend.position = "top",
        legend.justification = "right")

```

## Prior simulation

```{r}
all_dists %>%
  filter(group == "likelihood") %>%
  ggplot(aes(x = x, y = y, colour = group, fill = group)) +
  facet_wrap(~prior) +
  geom_vline(xintercept = 15, linetype = "dotted") +
  geom_line() +
  geom_ribbon(aes(ymax = y, ymin = 0), alpha = .25) +
  scale_fill_colorblind("") +
  scale_color_colorblind("") +
  scale_y_continuous(limits = c(0, 0.065)) +
  scale_x_continuous(limits = c(-90, 50)) +
  theme(legend.position = "top",
        legend.justification = "right")

```

## Prior simulation

```{r}
all_dists %>%
  filter(group %in% c("likelihood", "prior" )) %>%
  ggplot(aes(x = x, y = y, colour = group, fill = group)) +
  facet_wrap(~prior) +
  geom_vline(xintercept = 15, linetype = "dotted") +
  geom_line() +
  geom_ribbon(aes(ymax = y, ymin = 0), alpha = .25) +
  scale_fill_colorblind("") +
  scale_color_colorblind("") +
  scale_y_continuous(limits = c(0, 0.065)) +
  scale_x_continuous(limits = c(-90, 50)) +
  theme(legend.position = "top",
        legend.justification = "right")

```


## Prior simulation

```{r}
all_dists %>%
  ggplot(aes(x = x, y = y, colour = group, fill = group)) +
  facet_wrap(~prior) +
  geom_vline(xintercept = 15, linetype = "dotted") +
  geom_line() +
  geom_ribbon(aes(ymax = y, ymin = 0), alpha = .25) +
  scale_fill_colorblind("") +
  scale_color_colorblind("") +
  scale_y_continuous(limits = c(0, 0.065)) +
  scale_x_continuous(limits = c(-90, 50)) +
  theme(legend.position = "top",
        legend.justification = "right")

```




# The end


## For next session

> - We will compare different models of keystroke intervals (same data; 5 different probability models).
> - Run the following models and save their posterior:
 1. `gaussian.R`
 2. `lognormal.R`
 3. `exgaussian.R`
 4. `mixturemodel.R`
 5. `skewnormal.R` (spoiler: don't run this one, if you don't have time)
> - We need their posteriors for the next session.
> - Don't start this last minute :)


## References

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

